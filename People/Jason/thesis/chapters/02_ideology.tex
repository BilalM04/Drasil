The focus of this work is fundamentally based on the idea of ``generation.''
However, unlike GitHub and OpenAI's Copilot \cite{Copilot}, the ideology does
not delve into artificial intelligence, and does not focus on ``autocompleting
code'' by natural language. Instead of working with the set of natural language,
the ideology focuses on formalizing the meaning of specific subsets into
principled stories. The principled stories are then intertwined and mixed until
a coherent ``whole story'' is formed, where we deterministically understand what
can and cannot be done with the knowledge described (including, but not limited
to, generating representational software artifacts and snippets).

\section{On Developing Software}
\label{sec:idlgy:on_developing_software}

As a software developer working to build a new piece of software, one might
sense that they are writing ``a lot of the same code'' as their or other
pre-existing software projects. One might consider building a library, shared
between all projects for some common functionality/tooling, this is a large
improvement for their program --- being able to reuse their code is great for
debugging and removing the possibility of bugs when stable and tested code is
used. The library will surely save them time in the future once they've
stabilized it to a reasonable degree, after which they will not need to worry
about repeating the same errors they made when they were originally writing it.
They will have made significant gains in their development environment and
workflow. However, the library might not be portable across machines, the
library might not make sense to those more or less familiar with certain ideas
touched upon by the library, others might question the validity of the knowledge
it pulls from, and the library might not be readily accessible to those not
using the same programming language the library was written in. They can write
an \acs{ffi}, but this is a fairly complex task that many are unfamiliar with,
and which requires meticulous analysis to ensure compatibility and creates time
expensive update procedures when foreign libraries are updated.

Commonly, one looks to use mature libraries and frameworks to underpin their
projects, occasionally without guarantee that connecting these libraries is
safe. A familiar example of this failure is the sinking of the Vasa
\cite{wiki:Vasa_ship}, partially caused by different teams working together but
using different ``feet'' units (the Swedish foot is 12'' while the Amsterdam
foot is 11'') resulting in unintended weight distribution. As general purpose
programming languages are often also used, misunderstandings of tacit project
knowledge may also cause errors. Unfortunately, despite building on the
shoulders of giants, this programming methodology takes significant time and
stress until a working product is formed with minimal bugs.

To resolve this, we believe we need to revisit the original sensation felt when
code re-creation/duplication was recognized -- why does this sensation occur?

The reason is obvious (ignoring the more obvious fact that they are aware of
similarities between their code and that previously written)! They feel this
sensation because they already understood it and had written it previously.
Implicitly, when writing the software, the developer already had a mental
connection between some algorithm and the code they had written (at times,
specific to the programming language they had chosen to use) -- it might have
already been \textit{well-understood} to them.

Unfortunately, a non-trivial amount of time is spent manually recreating the
same piece of code from the same implicit knowledge-base, and the developer is
likely left with less than their preferred amount of time to work on the
components that are interesting/important that they might never have built
before or that they might not have connected together in the past.

\section{Thoughts of Generation}
\label{sec:idlgy:thoughts_of_generation}

The ideology foundational to this work is predicated on this simple idea that if
we understand how a \textit{thing} works, and we can create a working model
describing it, then we should be able to encode it, describing everything about
this model. Furthermore, if we understand how this \textit{thing} can be
transformed into another \textit{thing}, then we should be able to describe the
process of transformation as well. Finally, if we are able to do this en masse
to a pool of \textit{various things}, we should be able to generate whole pools
of \textit{other various things} from empty, or minimal seedling pools.

Applying this idea to the world of software, where we can create and model
\textit{things} (henceforth referred to as ``knowledge''), we can take pools of
knowledge and generate other pools of knowledge (including whole software
artifacts). \todo{Discuss triform theories} With a weak breadth and low depth of
captured knowledge, the originating pool of knowledge may approximately appear
as a description of the software artifacts generated. However, with a strong
breadth and high depth of captured knowledge, the original pool of knowledge may
appear far removed from the software artifacts generated (it may contain little
to no discussion of the desired software artifacts at all, or a precise
description). In this case, the stress of software development is alleviated
entirely, and a ``perfect'' software artifact (or series of ``perfect'' software
artifacts) is (are) constructed.

\section{The Goal}
\label{sec:idlgy:the_goal}

If one originally sets out to build a program that does something they
understand very well and each component of every step of the grand
scheme/algorithm of the program was understood, the development of their related
software should be a ``clear matter of principled engineering'' \todo{Cite
    GitHub README? I really don't remember where I read this now\ldots{}}. However,
with the existing programming methodology, it is not yet simple enough to
reliably produce error-free programs with no trade-offs, and which precisely
satisfy a set of requirements and follow a formal design.

Optimally, they would use their natural language to perfectly describe their
problem to their computer and have it magically give them a program that does
exactly what they wanted. Unfortunately, it is difficult to have computers
systematically understand and act on natural language as well as us humans can.
However, specific subsets of natural language may be usable.

\section{Reconciliation â€” ``Generate Everything''}
\label{sec:idlgy:generate_everything}

While it is difficult to have a computer act on a huge set of natural language,
we do have well-understood thoughts on specific \textit{terms} and how they can
be interpreted. More specifically, there are sub-languages of natural language
that we can use to describe specific kinds of knowledge, and problems and
solutions. Here, we recognize that selecting or creating \aclp{dsl} for specific
buckets of knowledge and creating \aclp{dsl} that connect buckets of other
\aclp{dsl}, we can effectively create rigid sub-languages of our natural
language used to describe programs. These rigid sub-languages being encoded as
\aclp{dsl} will have a well-defined and formalized \acs{ast}, which allows us to
write interpreters that can take them and transform them into other fragments of
knowledge (us being most interested in ultimately generating software
artifacts). Thus, with enough effort, and through sequencing and connecting
terms of \aclp{dsl}, we can effectively model what the discussed software
developers are trying to build, allowing the computer to be able to better
understand what they are trying to build.

Note, however, that this idea likely only thrives in domains of knowledge that
is ``well-understood'' \cite{well-understood}.

In theory, this should appear as a \ACF{kms}, where generation phases are passed
through the knowledge registry until a final knowledge registry is constructed
such that it satisfies the requirements of the user (e.g., generating some
desired software artifacts).

\section{Prospective Workflow \& Roles}
\label{sec:idlgy:prospective_workflow}

\intodo{Discuss ``idealized'' workflow.}

Following this ideology, there will be at least 3 key roles: the
\textbf{knowledge encoder}, the \textbf{knowledge user}, and the
\textbf{end-user} of the produced software artifact.

\subsection{Knowledge Encoder (Domain Expert)}

The knowledge encoder should be a master of a particular domain. They are
expected to encode the knowledge discussed in their respective domain in such a
way that is accessible to those without knowledge of their domain. Additionally,
they should encode information about the ways in which the knowledge can be
transformed into other forms of knowledge (including that which is
interdisciplinary). The knowledge they would be encoding should be as well-known
and globally standardized as possible. As discussed in
\autoref{sec:idlgy:generate_everything}, it is likely that the knowledge encoder
will focus on writing a series of highly specific \aclp{dsl}, where the
languages may be restricted to as specific as one term or a handful.

\subsection{Knowledge / Domain User \& Orchestrator}

The knowledge user/orchestrator should at least be familiar with a particular
domain, and have goals in mind for information that they would like to ``seed''
into their \acl{kms}. They should also have a working understanding of what the
end-user needs. From this, they should be able to connect the work of the domain
expert into ``plug-n-play'' stories (arguably, compilers for the end-users to
use), or be able to encode/reduce friction between knowledge encoded by domain
experts.

\subsection{End-user}

The final end-user should find the most delight from this ideology. They are the
actual users of the software artifacts, perhaps tweaking the final build of the
software artifacts to be accustomed to their workflow. If the tasks assigned to
the knowledge encoders and the knowledge users are performed correctly, then the
end-user should have strong confidence in the artifacts as they were built with
strict adherence to the knowledge captured at \textit{every step of the way}. As
such, one should confidently expect the final software artifacts to be
completely devoid of unexpected things (including errors, unconformities to
specifications, etc.).

\intodo{Figure out where this below paragraph should belong}
As a domain expert transcribing knowledge encodings of some well-understood
domain, one will largely be discussing the ways in which pieces of knowledge are
\textit{constructed} and \textit{relate to each other}. In order for this
abstract knowledge encodings to be \textit{usable} in some way, it is vital to
have ``names'' (\textit{types}) for the knowledge encodings. In working to
capture the working knowledge of a domain, it's of utmost importance to ensure
that all ``instances'' of your ``names'' (types) are \textit{always} usable in
some meaningful way and that the knowledge is exposed in a usable way (e.g.,
sufficiently through some sort of \acs{api}). In other words, all knowledge
encodings should create a stringent, explicit set of rules for which all
``instances'' should conform to, and, arguably, also creates a justification for
the need to create that particular knowledge/data type. As such, optimally, a
domain expert would write their knowledge encodings and renderers in a general
purpose programming language with a sound type system (e.g., Haskell
\cite{Haskell2010}, Agda \cite{Norell2007}, etc.) â€” preferring ones with a type
system based on formal type theories for their feature richness.

\section{Feasibility}
\label{sec:idlgy:feasibility}

In order for us to discuss feasibility of this idealized prospective workflow,
we must discuss the depth \& breadth of knowledge we need to make this feasible.
Depth of knowledge refers to the vertical knowledge understood about a specific
fragment of knowledge, and it's preciseness. For example, we may have a
low-depth of knowledge \& claim that English sentences are a sequence of
characters, or we might have a slightly ``deeper'' depth/understanding  of
sentences by describing them as a language that follows a specific syntax rule
set and using a specific set of words. Breadth of knowledge is the horizontal
domain of knowledge, it is the various kinds of knowledge we have in a wide
variety of subjects and domains.

In low-depth areas, we may observe that this ideology is very practical, and
heavily used. Widely used \ACFP{cms}, such as WordPress \cite{WordPress} and
Drupal \cite{Drupal}, and web frameworks, such as Django \cite{Django} and
Laravel \cite{Laravel}, are arguably also following similar ideals as this
ideology. They all typically provide a basic understanding of ``users'' of a
hosted website, facilities to write HTML content in one way or another, plugins,
and more. While some might be, these listed above are not specific to one
specific use-case. They are very versatile and highly reusable for a wide
variety of use-cases because they ship with low but sufficient depth of
knowledge (though they might call it ``features''). Out of the box, these web
technologies listed come with simple, common, functionality (features) and
powerful extensibility through either plugins or through software extension and
usage. With the basic tooling provided, users are able to rapidly deploy
websites with content. Through extending the website's knowledge-base (e.g.,
plugins or software extension), they are able to obtain a wider breadth and
deeper depth to the knowledge contained within them. Through this, the end-users
are able to encode increasingly complex and different kinds of data into the
systems to ultimately obtain increasingly specialized websites, such as
technical blogs, eCommerce websites, online accounting software, online
discussion forums, and more. In these technologies, knowledge depth typically
remains ``shallow'', but through increasing breadth of knowledge, increasingly
interesting websites may be created. The mechanized generation-related
components of the ideology is also fairly shallow in this area, but, still,
highly feasible.

\intodo{Realistically, almost any area of generation can be a ``low-depth''/breadth area too}

\intodo{Add to high knowledge density examples: \url{https://en.wikipedia.org/wiki/Algebraic_modeling_language}}
\intodo{Add to high knowledge density examples: \url{https://github.com/McMasterU/HashedExpression}}

In areas of high knowledge density, as long as developers have infinite patience
and can invest infinite time into transcribing knowledge and information into
the software, anything is possible, and this ideology is very practical.
Unfortunately, that situation is not quite realistic. As such, we should
restrict our scope in high knowledge density areas to only those
``well-understood'' \cite{well-understood}. \todo{Define well-understood}. In
projects with high knowledge density, it's crucial to have clear and concise
knowledge fragment encodings that allow users to directly interact with the
decomposed simplicity associated with each ``transformation'' operation.
Thankfully, since these domains are also typically codified (or easily
codifiable), this is doable. Through capturing many series of incomplex
transformations of knowledge into other forms, we are able to sequence and
compose them until an ultimate large, normally complex and complicated,
transformation is formed. Finally, through creating a directed discussion of
ideas, we can form a stable knowledge-base from which we can draw information.
One large feasible goal from it is to draw out usable software artifacts from it
by poking and prodding in all areas needed until we can deterministically form
them, as our needs demand.

The essence of this ideology lies in naturally obtaining mechanization
techniques through formalizing \textit{everything}. The ideology forces us to
question modern software development practices: cognitive stress and normal
errors aside, if a developer doesn't \textit{truly understand} the knowledge
they are encoding in a software product, then it should be normal to expect
logical issues and an ``imperfect'' program. Taking cognitive stress into
consideration, there should be considerably less as knowledge only need be
transcribed once, and re-used infinitely. Through mechanization, cognitive
stress of re-writing knowledge is alleviated afterwards for all proceeding
instances. A formalized-knowledge-first approach to software development should
highlight areas of issue (poor understanding), never produce bugs, and create
software with the same quality as the encoded knowledge.

\intodo{The ideology also relates to the question of ``which programming
    language do I pick for my project?''. This is a moot question under this
    ideology. There is no one (1) single language that we even use to describe
    everything, so how can we possibly write all kinds of software for all kinds
    of knowledge? In a sense, the ideology ``leans into it'' by saying you
    shouldn't be limited to choosing just one (1).}

\intodo{The ideology demands that we unify modern domain knowledge. Almost
    naturally, we obtain mechanization as a side effect. In other words, it
    demands that we connect all of our compilers into one single coherent
    mega-compiler system.}

\intodo{One of the large issues with modern software development is a disconnect
    in knowledge, which is why this paradigm is the natural answer. The
    ideology is, exactly, ``unify all your knowledge.'' Everything gained by
    unifying our knowledge is a natural side effect, which we happen to care
    about.}

\intodo{Through describing their requirements in a coherent manner (e.g., some
    language), completely non-technical product owners may, and will, still be
    key figures in the production of the product.}
